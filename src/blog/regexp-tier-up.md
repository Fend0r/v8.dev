---
title: 'Improving V8 regular expressions'
author: 'Patrick Thier and Ana Pe≈°ko'
date: 2019-09-30 11:24:16
tags:
  - internals
  - RegExp
description: 'In this blog post we describe how we take advantage of the upsides of interpreting regular expressions and mitigate the downsides.'
---
In its default configuration, V8 compiles regular expressions to native code upon the first execution. With the introduction of [JIT-less V8](/blog/jitless), an interpreter for regular expressions was introduced. Interpreting regular expressions has the advantage of using less memory, but it comes with a performance penalty. In this blog post we describe how we take advantage of the upsides of interpreting regular expressions and mitigate the downsides.

## Tier-up strategy for RegExp

TODO(anapesko)

## Speeding up the RegExp interpreter

### Remove runtime overhead

When a regular expression is executed, a built-in written in [CodeStubAssembler](/blog/csa) is invoked. This built-in previously checked if the JSRegExp objects code field contains JITted native code that can be executed directly, and otherwise called a runtime method to compile (or interpret in JIT-less mode) the RegExp. Thus in JIT-less mode, every execution of a regular expression went through the V8 runtime, which is quite expensive.
Starting with V8 v7.8, whenever the RegExp compiler generates bytecode to interpret a regular expression, a trampoline to the RegExp interpreter is now stored in the JSRegExp objects code field in addition to the generated bytecode. This way the interpreter now gets called from the built-in directly without a detour through the runtime.

### New dispatch method

The RegExp interpreter previously used a simple switch-based dispatch method. The main disadvantage of this method is that the CPU has a very hard time predicting the next bytecode to execute, resulting in many branch mispredictions, slowing down execution.
We changed the dispatch method to threaded code in V8 v7.8. This method allows the CPUs branch predictor to predict the next bytecode based on the currently executed bytecode, resulting in less mispredictions. In more detail, we use a dispatch table, storing for each bytecode ID the address of the handler implementing the bytecode. V8's interpreter [Ignition](/docs/ignition) also implements this method. However a big difference between Ignition and the RegExp interpreter is that Ignition's bytecode handlers are written in [CodeStubAssembler](/blog/csa), whereas the RegExp interpreter uses [computed gotos](https://gcc.gnu.org/onlinedocs/gcc/Labels-as-Values.html) (a GNU extension also supported by clang). Thus the whole interpreter is written in C++, which is easier to read and maintain than CSA. For compilers that don't support computed gotos, we fallback to the old switch-based dispatch method.

### Bytecode peephole optimization

Before we talk about bytecode peephole optimization let's look at a motivating example.

```js
const re = /[^_]*/;
const str = 'a0b*c_ef';
re.exec(str);  // matches 'a0b*c'
```

For this simple pattern, the RegExp compiler creates 3 bytecodes that are executed for every character. On a high level these are:

1. Load current character.
2. Check if character equals '_'.
3. If not, advance current position in the subject string and goto (1).

So for our subject string we interpret 17 bytecodes until we find a non-matching character.
The idea of peephole optimization is that we replace sequences of bytecodes with a new optimized bytecode that combines the functionality of multiple bytecodes. In our example we can even handle the implicit loop created by the goto explicitly in the new bytecode, thus a single bytecode handles all matching characters, saving 16 dispatches.
Although the example is made up, the sequence of bytecodes described here occurs frequently in real world websites. We analyzed [real websites](/blog/real-world-performance) and created new optimized bytecodes for the most frequent bytecode sequences we encountered.

## Results

TODO(anapesko)

<figure>
  <img src="/_img/regexp-tier-up/results_speed.svg" width="600" height="371" alt="" loading="lazy">
  <figcaption>Figure 1: RegExp performance comparison</figcaption>
</figure>

Figure 1 shows the impact on the performance of the RegExp interpreter for all improvements described in this blog post[^strict-bounds] on the RexBench benchmark suite. For reference the performance of JIT compiled RegExp is also shown (Native).
The new interpreter is up to 2x as fast as the old one, averaging about 1.45x as fast. We even come quite close to the performance of JITted RegExp for most benchmarks, with Regex DNA being the only exception. The reason why interpreted RegExp are that much slower than JITted RegExp on this benchmark is due to the long subject strings (~300.000 characters) used. Even though we reduced dispatch overhead to a minimum, the overhead sums up on strings with more than 1.000 characters, resulting in slower execution.

[^strict-bounds]: The results shown here also include an improvement to regular expressions already described in the [V8 v7.8 release notes](/blog/v8-release-78#faster-regexp-match-failures).
